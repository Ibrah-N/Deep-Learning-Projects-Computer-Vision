{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMTVksKjrnUHGIKnc4d8F7A",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ibrah-N/Deep-Learning-Projects-Computer-Vision/blob/main/dl_18_yolo_object_detection_model_scratch_with_yolo_custom_loss.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "WhxWNuUgBtXL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRWPx3bNBAGr",
        "outputId": "5f27be06-d877-42d9-a531-4b70b3778b65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "56uvzff-Bv0f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing Data"
      ],
      "metadata": {
        "id": "m9JqnCXnBwoC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download Dataset"
      ],
      "metadata": {
        "id": "UGPnu26YB0yd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install kaggle\n",
        "\n",
        "# !pip install -q kaggle"
      ],
      "metadata": {
        "id": "deIl-_AnBvxL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the kaggle.json to kaggle\n",
        "\n",
        "# !mkdir ~/.kaggle\n",
        "# !cp kaggle.json ~/.kaggle/\n",
        "# !chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "5TsPC9sQCfZN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# donwload kaggle dataset\n",
        "\n",
        "# !kaggle datasets download -d huanghanchina/pascal-voc-2012\n",
        "# !unzip pascal-voc-2012.zip -d dataset"
      ],
      "metadata": {
        "id": "UxkIRUn-Bvtz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Copy the dataset to drive\n",
        "\n",
        "# !cp -r /content/pascal-voc-2012.zip /content/drive/MyDrive/\n",
        "\n",
        "\n",
        "!unzip -q /content/drive/MyDrive/pascal-voc-2012.zip -d dataset"
      ],
      "metadata": {
        "id": "ubBJ0u5CAkC5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configuration"
      ],
      "metadata": {
        "id": "n-8jdHwDB4Pc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_IMAGES = \"/content/dataset/VOC2012/JPEGImages/\"\n",
        "DATASET_MAPS = \"/content/dataset/VOC2012/Annotations/\"\n",
        "\n",
        "\n",
        "\n",
        "CLASSES = ['aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus', 'car', 'cat', 'chair', 'cow', 'diningtable',\n",
        "           'dog', 'horse', 'motorbike', 'person', 'pttedplant', 'sheep', 'sofa', 'train', 'tvmonitor']\n",
        "\n",
        "\n",
        "\n",
        "B = 2\n",
        "N_CLASSES = len(CLASSES)\n",
        "OBJ_IND = N_CLASSES+5*B\n",
        "H, W = 224, 224\n",
        "SPLIT_SIZE = int(H/32)\n",
        "N_EPOCHS = 25\n",
        "LR = 0.00001\n",
        "BATCH_SIZE = 32"
      ],
      "metadata": {
        "id": "Md6yy-pdBvrH"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir /content/dataset/VOC2012/VAL_JPEGImages/\n",
        "!mkdir /content/dataset/VOC2012/VAL_Annotations/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDPAT9kqRMbP",
        "outputId": "b71d2b72-be0a-42a5-d9da-08528ad3b9c0"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/dataset/VOC2012/VAL_JPEGImages/’: File exists\n",
            "mkdir: cannot create directory ‘/content/dataset/VOC2012/VAL_Annotations/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VAL_IMAGES = \"/content/dataset/VOC2012/VAL_JPEGImages/\"\n",
        "VAL_MAPS = \"/content/dataset/VOC2012/VAL_Annotations/\"\n",
        "\n",
        "\n",
        "\n",
        "val_list = [\n",
        "    \"2007_000027\", \"2007_000032\", \"2007_000033\", \"2007_000123\", \"2007_000559\", \"2007_000664\", \"2007_000676\",\n",
        "    \"2007_000584\", \"2007_001073\", \"2007_001154\", \"2007_000925\", \"2007_001761\", \"2007_001724\", \"2007_001763\",\n",
        "    \"2007_002445\", \"2007_002462\", \"2007_002488\", \"2007_002539\", \"2007_002545\", \"2007_002387\", \"2007_002400\",\n",
        "]"
      ],
      "metadata": {
        "id": "ZRQnU2NlRvq9"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# move the validattion images\n",
        "# and xmls into val directory\n",
        "\n",
        "for val in val_list:\n",
        "  shutil.move(DATASET_IMAGES+val+\".jpg\", VAL_IMAGES)\n",
        "\n",
        "\n",
        "for val in val_list:\n",
        "  shutil.move(DATASET_MAPS+val+\".xml\", VAL_MAPS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "id": "cpnz_WIgTRAB",
        "outputId": "9b88f717-c4bf-4d4e-96c8-76c882981241"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "error",
          "ename": "Error",
          "evalue": "Destination path '/content/dataset/VOC2012/VAL_JPEGImages/2007_000027.jpg' already exists",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-7ce36d283343>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_IMAGES\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".jpg\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVAL_IMAGES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/shutil.py\u001b[0m in \u001b[0;36mmove\u001b[0;34m(src, dst, copy_function)\u001b[0m\n\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 814\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Destination path '%s' already exists\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    815\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    816\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_dst\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mError\u001b[0m: Destination path '/content/dataset/VOC2012/VAL_JPEGImages/2007_000027.jpg' already exists"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# parse the xml tree elements\n",
        "\n",
        "def parse_xml(filename):\n",
        "  tree = ET.parse(filename)\n",
        "  root = tree.getroot()\n",
        "  size_tree = root.find('size')\n",
        "  width = int(size_tree.find('width').text)\n",
        "  height = int(size_tree.find('height').text)\n",
        "\n",
        "\n",
        "  bounding_boxes = []\n",
        "  for obj in root.findall('object'):\n",
        "    for box in obj.iter('bndbox'):\n",
        "      xmin = int(box.find('xmin').text)\n",
        "      ymin = int(box.find('ymin').text)\n",
        "      xmax = int(box.find('xmax').text)\n",
        "      ymax = int(box.find('ymax').text)\n",
        "      break\n",
        "\n",
        "\n",
        "    class_name = obj.find('name').text\n",
        "    class_dict = {CLASSES[i]:i for i in range(len(CLASSES))}\n",
        "    box = [\n",
        "        (xmin+xmax)/(2*width), (ymin+ymax)/(2*height),\n",
        "        (xmax-xmin)/(width), (ymax-ymin)/(height),\n",
        "        class_dict[class_name]\n",
        "    ]\n",
        "    bounding_boxes.append(box)\n",
        "\n",
        "\n",
        "  return np.array(bounding_boxes, dtype=np.float32)"
      ],
      "metadata": {
        "id": "-6I05zUvBvoI"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "boundingbox = parse_xml(DATASET_MAPS+\"2007_000061.xml\")\n",
        "boundingbox"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cChBsqnAZrJ",
        "outputId": "d3fe132b-d3a8-421d-c24b-36d7a7583aaa"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.711     , 0.43543544, 0.326     , 0.8048048 , 3.        ],\n",
              "       [0.465     , 0.6996997 , 0.194     , 0.11411411, 3.        ]],\n",
              "      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def output labels\n",
        "\n",
        "def output_labels(bbox):\n",
        "  labels = np.zeros((SPLIT_SIZE,SPLIT_SIZE, int(N_CLASSES+5)))\n",
        "\n",
        "  for b in range(len(bbox)):\n",
        "    print(bbox)\n",
        "    grid_x = bbox[...,b, 0] * SPLIT_SIZE\n",
        "    grid_y = bbox[...,b, 1] * SPLIT_SIZE\n",
        "    i = int(grid_x)\n",
        "    j = int(grid_y)\n",
        "\n",
        "    labels[i, j, 0:5] = [1, grid_x%1, grid_y%1, bbox[...,b, 2], bbox[...,b, 3]]\n",
        "    labels[i, j, 5+int(bbox[...,b, 4])] = 1\n",
        "\n",
        "\n",
        "  return tf.convert_to_tensor(labels, dtype=tf.float32)"
      ],
      "metadata": {
        "id": "aY252dxnMbuD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_lbl = output_labels(boundingbox)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Xyw3kCNujw",
        "outputId": "af8beff1-8359-4f26-fd9c-168364b434d8"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.711      0.43543544 0.326      0.8048048  3.        ]\n",
            " [0.465      0.6996997  0.194      0.11411411 3.        ]]\n",
            "[[0.711      0.43543544 0.326      0.8048048  3.        ]\n",
            " [0.465      0.6996997  0.194      0.11411411 3.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output_lbl.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiPS9Ee_dBbI",
        "outputId": "dbcbed4a-7f5b-4ab9-f74a-c073771d8ac0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([7, 7, 25])"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = cv.imread(DATASET_IMAGES+\"2007_000032.jpg\")"
      ],
      "metadata": {
        "id": "Ay4aLG2uAZom"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "kHDYsmOwB-lD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "img_paths = []\n",
        "xml_paths = []\n",
        "for path in os.listdir(DATASET_IMAGES):\n",
        "  img_paths.append(DATASET_IMAGES+path)\n",
        "  xml_paths.append(DATASET_MAPS+path.split(\".\")[0]+\".xml\")\n",
        "\n",
        "\n",
        "\n",
        "val_img_paths = []\n",
        "val_xml_paths = []\n",
        "for path in os.listdir(VAL_IMAGES):\n",
        "  val_img_paths.append(VAL_IMAGES+path)\n",
        "  val_xml_paths.append(VAL_MAPS+path.split(\".\")[0]+\".xml\")\n",
        "\n",
        "\n",
        "\n",
        "print(f\"Total Images: {len(img_paths)}\")\n",
        "print(f\"Total Validation Images: {len(val_img_paths)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QywNxkBVBvlr",
        "outputId": "8388026f-c17a-4c5f-ae19-677bf1bc92a8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Images: 17104\n",
            "Total Validation Images: 21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((img_paths, xml_paths))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((val_img_paths, val_xml_paths))"
      ],
      "metadata": {
        "id": "R2lQvweLFL33"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FmYzE0dqFL0P"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design Dataset"
      ],
      "metadata": {
        "id": "SaU-3RcTFNfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_img_box(img_path, xml_path):\n",
        "\n",
        "  img = tf.io.decode_jpeg(tf.io.read_file(img_path))\n",
        "  img = tf.cast(tf.image.resize(img, (H, W)), dtype=tf.float32)\n",
        "\n",
        "  bbox = tf.numpy_function(func=parse_xml, inp=[xml_path], Tout=tf.float32)\n",
        "  return img, bbox"
      ],
      "metadata": {
        "id": "VemCgT9ZFLx7"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .map(get_img_box)\n",
        ")\n",
        "\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(get_img_box)\n",
        ")"
      ],
      "metadata": {
        "id": "wAf308LPFLvZ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_labels(img, bbox):\n",
        "  labels = tf.numpy_function(func=output_labels, inp=[bbox], Tout=tf.float32)\n",
        "  return img, labels"
      ],
      "metadata": {
        "id": "EpP7EiHkBvjK"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    train_dataset\n",
        "    .map(process_labels)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")\n",
        "\n",
        "\n",
        "val_dataset = (\n",
        "    val_dataset\n",
        "    .map(process_labels)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .prefetch(tf.data.AUTOTUNE)\n",
        ")"
      ],
      "metadata": {
        "id": "AXMLnBq044sX"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Yolo Loss"
      ],
      "metadata": {
        "id": "96kKwMTXCAT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def IoU(boxes1, boxes2):\n",
        "  boxes1_t = tf.stack([boxes1[..., 0] - boxes1[..., 2] / 2.0,\n",
        "                       boxes1[..., 1] - boxes1[..., 3] / 2.0,\n",
        "                       boxes1[..., 0] + boxes1[..., 2] / 2.0,\n",
        "                       boxes1[..., 1] + boxes1[..., 3] / 2.0],\n",
        "                       axis=-1)\n",
        "  boxes2_t = tf.stack([boxes2[..., 0] - boxes2[..., 2] / 2.0,\n",
        "                       boxes2[..., 1] - boxes2[..., 3] / 2.0,\n",
        "                       boxes2[..., 0] + boxes2[..., 2] / 2.0,\n",
        "                       boxes2[..., 1] + boxes2[..., 3] / 2.0],\n",
        "                       axis=-1)\n",
        "\n",
        "  lu = tf.maximum(boxes1_t[..., :2], boxes2_t[..., :2])\n",
        "  rd = tf.minimum(boxes1_t[..., 2:], boxes2_t[..., 2:])\n",
        "\n",
        "  intersection = tf.maximum(0.0, rd - lu)\n",
        "  intersection_area = intersection[..., 0] * intersection[..., 1]\n",
        "  boxes1_area = boxes1[..., 2] * boxes1[..., 3]\n",
        "  boxes2_area = boxes2[..., 2] * boxes2[..., 3]\n",
        "\n",
        "  union_area = tf.maximum(boxes1_area + boxes2_area - intersection_area, 1e-8)\n",
        "  return tf.clip_by_value(intersection_area / union_area, 0.0, 1.0)"
      ],
      "metadata": {
        "id": "G3jLBi6ZQD4R"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def differences(y_true, y_pred):\n",
        "  return tf.reduce_sum(tf.square(y_true - y_pred))"
      ],
      "metadata": {
        "id": "Nq-zLHg12d9U"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Yolo Model Loss\n",
        "# 1.  Coordinates Loss\n",
        "# 2.  Size Loss\n",
        "# 3.  Objectness Loss\n",
        "# 4.  No Objectness Loss\n",
        "# 5.  Classes Loss\n",
        "\n",
        "def yolo_loss(y_true, y_pred):\n",
        "  target = y_true[..., 0]\n",
        "\n",
        "  ########## Objectness Loss ##############\n",
        "  y_target_extract = tf.gather_nd(y_true, tf.where(target[:]==1))\n",
        "  y_pred_extract = tf.gather_nd(y_pred, tf.where(target[:]==1))\n",
        "\n",
        "  print(tf.where(target[:]==1))\n",
        "  print(y_target_extract)\n",
        "  print(y_pred_extract)\n",
        "\n",
        "\n",
        "  rescalar = tf.where(target[:]==1)*32\n",
        "  upscalar_1 = tf.concat([rescalar[:, 1:], tf.zeros([len(rescalar), 2], dtype=tf.int64)], axis=-1)\n",
        "\n",
        "  target_upscalar_2 = tf.repeat([[32., 32., 224., 224.]],\n",
        "                                repeats=[len(rescalar)], axis=0)*tf.cast(y_target_extract[...,1:5], dtype=tf.float32)\n",
        "  pred_1_upscalar_2 = tf.repeat([[32., 32., 224., 224.]],\n",
        "                                repeats=[len(rescalar)], axis=0)*tf.cast(y_pred_extract[...,1:5], dtype=tf.float32)\n",
        "  pred_2_upscalar_2 = tf.repeat([[32., 32., 224., 224.]],\n",
        "                                repeats=[len(rescalar)], axis=0)*tf.cast(y_pred_extract[...,6:10], dtype=tf.float32)\n",
        "\n",
        "  target_orig = tf.cast(upscalar_1, dtype=tf.float32)+tf.cast(target_upscalar_2, dtype=tf.float32)\n",
        "  pred_1_orig = tf.cast(upscalar_1, dtype=tf.float32)+tf.cast(pred_1_upscalar_2, dtype=tf.float32)\n",
        "  pred_2_orig = tf.cast(upscalar_1, dtype=tf.float32)+tf.cast(pred_2_upscalar_2, dtype=tf.float32)\n",
        "\n",
        "  mask = tf.cast(tf.math.greater(IoU(target_orig, pred_1_orig), IoU(target_orig, pred_2_orig)), dtype=tf.int32)\n",
        "  y_pred_joined = tf.transpose(tf.concat([tf.expand_dims(y_pred_extract[..., 0], axis=0),\n",
        "                                         tf.expand_dims(y_pred_extract[...,5], axis=0)],\n",
        "                                         axis=0))\n",
        "  obj_pred = tf.gather_nd(y_pred_joined, tf.stack([tf.range(len(rescalar)), mask], axis=-1))\n",
        "  object_loss = differences(tf.cast(obj_pred, dtype=tf.float32), tf.cast(tf.ones([len(rescalar)]), dtype=tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "  ########## No Objectness Loss ##############\n",
        "  y_pred_extract = tf.gather_nd(y_pred[...,0:B*5], tf.where(target[:]==0))\n",
        "  y_target_extract = tf.zeros(len(y_pred_extract))\n",
        "\n",
        "  no_object_loss_1 = differences(tf.cast(y_pred_extract[..., 0], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32))\n",
        "  no_object_loss_2 = differences(tf.cast(y_pred_extract[..., 5], dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32))\n",
        "  no_object_loss = no_object_loss_1 + no_object_loss_2\n",
        "\n",
        "\n",
        "\n",
        "  ########## Classes Loss ##############\n",
        "  y_pred_extract = tf.gather_nd(y_pred[..., B*5:], tf.where(target[:]==1))\n",
        "  y_target_extract = tf.gather_nd(y_true[..., 5:], tf.where(target[:]==1))\n",
        "  class_loss = differences(tf.cast(y_pred_extract, dtype=tf.float32), tf.cast(y_target_extract, dtype=tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "  ########## Center Co-ordinates Loss ##############\n",
        "  y_pred_extract = tf.gather_nd(y_pred[..., 0:B*5], tf.where(target[:]==1))\n",
        "  center_joined = tf.stack([y_pred_extract[...,1:3], y_pred_extract[...,6:8]], axis=1)\n",
        "\n",
        "  center_pred = tf.gather_nd(center_joined, tf.stack([tf.range(len(rescalar)), mask], axis=-1))\n",
        "  center_target = tf.gather_nd(y_true[..., 1:3], tf.where(target[:]==1))\n",
        "  center_loss = differences(tf.cast(center_pred, dtype=tf.float32), tf.cast(center_target, dtype=tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  ########## Size Co-ordinates Loss ##############\n",
        "  size_joined = tf.stack([y_pred_extract[...,3:5], y_pred_extract[...,8:10]], axis=1)\n",
        "\n",
        "  size_pred = tf.gather_nd(size_joined, tf.stack([tf.range(len(rescalar)), mask], axis=-1))\n",
        "  size_target = tf.gather_nd(y_true[..., 3:5], tf.where(target[:]==1))\n",
        "\n",
        "  size_loss = differences(tf.cast(tf.math.sqrt(tf.math.abs(size_pred)), dtype=tf.float32), tf.cast(tf.math.sqrt(tf.math.abs(size_target)), dtype=tf.float32))\n",
        "  box_loss = center_loss + size_loss\n",
        "\n",
        "\n",
        "  lambda_coord = 5.0\n",
        "  lambda_no_obj = 0.5\n",
        "  loss = (\n",
        "         tf.cast(object_loss, dtype=tf.float32)\n",
        "       + tf.cast((lambda_no_obj*no_object_loss), dtype=tf.float32)\n",
        "       + tf.cast((lambda_coord*box_loss), dtype=tf.float32)\n",
        "       + tf.cast(class_loss, dtype=tf.float32)\n",
        "      )\n",
        "\n",
        "\n",
        "  return loss"
      ],
      "metadata": {
        "id": "a2yUKbMiBvhE"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = output_labels(np.array([[ 0.163     ,  0.25866666,  0.05      ,  0.08      , 14.        ],\n",
        "                                 [ 0.787     ,  0.31866667,  0.078     ,  0.12533334, 14.        ]]))\n",
        "y_true = tf.expand_dims(y_true, axis=0)\n",
        "\n",
        "\n",
        "y_pred = np.random.normal(size = (1, SPLIT_SIZE, SPLIT_SIZE, N_CLASSES+10))\n",
        "y_pred[0][1][1] = [0.9, 0.15, 0.22, 0.04, 0.06,    0.5, 0.11, 0.1, 0.01, 0.02, 0.8,  0.12,0.0,0.,0.2,0.1,0.5,0.7,0.21,0.32,0.7,0.1,0.4,0.3,1.,0.12,0.0,0.,0.2,0.1]\n",
        "y_pred[0][5][2] = [0.9, 0.77, 0.3, 0.04, 0.06,    0.8, 0.75, 0.3, 0.075, 0.02, 0.1,  0.1,0.1,0.2,0.,0.21,0.45,0.45,0.9,0.12,0.45,0.7,0.,0.,1.,0.12,0.0,0.,0.2,0.1]\n",
        "\n",
        "yolo_loss(y_true, y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JLlAeXU8awP",
        "outputId": "0b5664d5-6774-470b-f356-c60b85c29efb"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.163       0.25866666  0.05        0.08       14.        ]\n",
            " [ 0.787       0.31866667  0.078       0.12533334 14.        ]]\n",
            "[[ 0.163       0.25866666  0.05        0.08       14.        ]\n",
            " [ 0.787       0.31866667  0.078       0.12533334 14.        ]]\n",
            "tf.Tensor(\n",
            "[[0 1 1]\n",
            " [0 5 2]], shape=(2, 3), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[1.         0.141      0.8106666  0.05       0.08       0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.        ]\n",
            " [1.         0.509      0.2306667  0.078      0.12533334 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         1.         0.         0.         0.         0.\n",
            "  0.        ]], shape=(2, 25), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.9   0.15  0.22  0.04  0.06  0.5   0.11  0.1   0.01  0.02  0.8   0.12\n",
            "  0.    0.    0.2   0.1   0.5   0.7   0.21  0.32  0.7   0.1   0.4   0.3\n",
            "  1.    0.12  0.    0.    0.2   0.1  ]\n",
            " [0.9   0.77  0.3   0.04  0.06  0.8   0.75  0.3   0.075 0.02  0.1   0.1\n",
            "  0.1   0.2   0.    0.21  0.45  0.45  0.9   0.12  0.45  0.7   0.    0.\n",
            "  1.    0.12  0.    0.    0.2   0.1  ]], shape=(2, 30), dtype=float64)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float32, numpy=53.398106>"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8_KcwH-N8ath"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Edh6t-Z8aqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y0_NvUj98am-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom Yolo Model"
      ],
      "metadata": {
        "id": "OrjU_ZGOCG8_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint_filepath = \"Checkpoint/yolo_resnet_$0.h5\"\n",
        "callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    save_weights_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    save_best_only=True)"
      ],
      "metadata": {
        "id": "k0tk0bhqCGXR"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scheduler(epoch, lr):\n",
        "  if epoch < 10:\n",
        "    return lr\n",
        "  elif epoch >= 10 and epoch < 20:\n",
        "    return 0.000001\n",
        "  else:\n",
        "    return 0.0000001\n",
        "\n",
        "lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler)"
      ],
      "metadata": {
        "id": "Bn11-wGgCGUe"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_FILTERS = 512\n",
        "OUTPUT_DIMS = N_CLASSES+5*B"
      ],
      "metadata": {
        "id": "Rh_lSNugCGSM"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = tf.keras.applications.ResNet50(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(H, W, 3)\n",
        ")\n",
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "2vhPoU7uCGPg"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    tf.keras.layers.Conv2D(NUM_FILTERS, 3, padding=\"same\", kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(NUM_FILTERS, 3, padding=\"same\", kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "\n",
        "    tf.keras.layers.Conv2D(NUM_FILTERS, 3, padding=\"same\", kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "    tf.keras.layers.Conv2D(NUM_FILTERS, 3, padding=\"same\", kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "    tf.keras.layers.Flatten(),\n",
        "\n",
        "    tf.keras.layers.Dense(NUM_FILTERS, kernel_initializer='he_normal'),\n",
        "    tf.keras.layers.BatchNormalization(),\n",
        "    tf.keras.layers.LeakyReLU(alpha=0.1),\n",
        "\n",
        "    tf.keras.layers.Dense(SPLIT_SIZE*SPLIT_SIZE*OUTPUT_DIMS, activation='sigmoid'),\n",
        "    tf.keras.layers.Reshape((SPLIT_SIZE, SPLIT_SIZE, OUTPUT_DIMS))\n",
        "])\n",
        "yolo_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YMotjb5CGND",
        "outputId": "3e828269-f214-4f62-d128-b131bce31927"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 7, 7, 2048)        23587712  \n",
            "                                                                 \n",
            " conv2d_4 (Conv2D)           (None, 7, 7, 512)         9437696   \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 7, 7, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_5 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_5 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 7, 7, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_6 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_6 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " batch_normalization_6 (Bat  (None, 7, 7, 512)         2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_7 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " conv2d_7 (Conv2D)           (None, 7, 7, 512)         2359808   \n",
            "                                                                 \n",
            " leaky_re_lu_8 (LeakyReLU)   (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 25088)             0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 512)               12845568  \n",
            "                                                                 \n",
            " batch_normalization_7 (Bat  (None, 512)               2048      \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " leaky_re_lu_9 (LeakyReLU)   (None, 512)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1470)              754110    \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 7, 7, 30)          0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 53712702 (204.90 MB)\n",
            "Trainable params: 30120894 (114.90 MB)\n",
            "Non-trainable params: 23591808 (90.00 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=LR),\n",
        "    loss=yolo_loss\n",
        ")"
      ],
      "metadata": {
        "id": "FW1Eaw77PBq8"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = yolo_model.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=N_EPOCHS,\n",
        "    callbacks=[callback, lr_schedule]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qs8S_KM4Pwrb",
        "outputId": "a4830a1d-2a70-4f66-e13f-52a2a1393a76"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "Tensor(\"yolo_loss/Where_2:0\", shape=(None, None), dtype=int64)\n",
            "Tensor(\"yolo_loss/GatherNd:0\", dtype=float32)\n",
            "Tensor(\"yolo_loss/GatherNd_1:0\", dtype=float32)\n",
            "Tensor(\"yolo_loss/Where_2:0\", shape=(None, None), dtype=int64)\n",
            "Tensor(\"yolo_loss/GatherNd:0\", dtype=float32)\n",
            "Tensor(\"yolo_loss/GatherNd_1:0\", dtype=float32)\n",
            "[[0.659      0.50149703 0.682      0.997006   7.        ]]\n",
            "[[ 0.50149256  0.738       0.99701494  0.512       1.        ]\n",
            " [ 0.71791047  0.894       0.2716418   0.212      14.        ]]\n",
            "[[ 0.50149256  0.738       0.99701494  0.512       1.        ]\n",
            " [ 0.71791047  0.894       0.2716418   0.212      14.        ]]\n",
            "[[ 0.649       0.7506667   0.37        0.49866667 14.        ]]\n",
            "[[ 0.465      0.5828877  0.274      0.8342246 14.       ]]\n",
            "[[ 0.283       0.5315315   0.13        0.0960961   6.        ]\n",
            " [ 0.917       0.6156156   0.082       0.0960961  13.        ]\n",
            " [ 0.915       0.5570571   0.038       0.11111111 14.        ]]\n",
            "[[ 0.283       0.5315315   0.13        0.0960961   6.        ]\n",
            " [ 0.917       0.6156156   0.082       0.0960961  13.        ]\n",
            " [ 0.915       0.5570571   0.038       0.11111111 14.        ]]\n",
            "[[ 0.283       0.5315315   0.13        0.0960961   6.        ]\n",
            " [ 0.917       0.6156156   0.082       0.0960961  13.        ]\n",
            " [ 0.915       0.5570571   0.038       0.11111111 14.        ]]\n",
            "[[ 0.595       0.776       0.118       0.11733333 16.        ]]\n",
            "[[ 0.661       0.8183183   0.678       0.36336336 10.        ]\n",
            " [ 0.904       0.5945946   0.06        0.3063063   4.        ]\n",
            " [ 0.657       0.6411411   0.042       0.1891892   4.        ]\n",
            " [ 0.247       0.8303303   0.49        0.33933935  8.        ]\n",
            " [ 0.232       0.6186186   0.36        0.7207207  14.        ]]\n",
            "[[ 0.661       0.8183183   0.678       0.36336336 10.        ]\n",
            " [ 0.904       0.5945946   0.06        0.3063063   4.        ]\n",
            " [ 0.657       0.6411411   0.042       0.1891892   4.        ]\n",
            " [ 0.247       0.8303303   0.49        0.33933935  8.        ]\n",
            " [ 0.232       0.6186186   0.36        0.7207207  14.        ]]\n",
            "[[ 0.661       0.8183183   0.678       0.36336336 10.        ]\n",
            " [ 0.904       0.5945946   0.06        0.3063063   4.        ]\n",
            " [ 0.657       0.6411411   0.042       0.1891892   4.        ]\n",
            " [ 0.247       0.8303303   0.49        0.33933935  8.        ]\n",
            " [ 0.232       0.6186186   0.36        0.7207207  14.        ]]\n",
            "[[ 0.661       0.8183183   0.678       0.36336336 10.        ]\n",
            " [ 0.904       0.5945946   0.06        0.3063063   4.        ]\n",
            " [ 0.657       0.6411411   0.042       0.1891892   4.        ]\n",
            " [ 0.247       0.8303303   0.49        0.33933935  8.        ]\n",
            " [ 0.232       0.6186186   0.36        0.7207207  14.        ]]\n",
            "[[ 0.661       0.8183183   0.678       0.36336336 10.        ]\n",
            " [ 0.904       0.5945946   0.06        0.3063063   4.        ]\n",
            " [ 0.657       0.6411411   0.042       0.1891892   4.        ]\n",
            " [ 0.247       0.8303303   0.49        0.33933935  8.        ]\n",
            " [ 0.232       0.6186186   0.36        0.7207207  14.        ]]\n",
            "[[0.491      0.45945945 0.878      0.4084084  0.        ]]\n",
            "[[ 0.497      0.4804217  0.986      0.936747  13.       ]]\n",
            "[[0.501      0.28912467 0.998      0.5729443  6.        ]\n",
            " [0.52       0.48010612 0.96       0.80106103 1.        ]]\n",
            "[[0.501      0.28912467 0.998      0.5729443  6.        ]\n",
            " [0.52       0.48010612 0.96       0.80106103 1.        ]]\n",
            "[[0.285      0.31586826 0.126      0.2245509  2.        ]]\n",
            "[[ 0.483       0.472       0.614       0.65066665 12.        ]]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnknownError",
          "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  KeyError: 'pottedplant'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-42-54d41e2b71be>\", line 26, in parse_xml\n    class_dict[class_name]\n\nKeyError: 'pottedplant'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[yolo_loss/cond_1/else/_13/yolo_loss/cond_1/StringJoin/_92]]\n  (1) UNKNOWN:  KeyError: 'pottedplant'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-42-54d41e2b71be>\", line 26, in parse_xml\n    class_dict[class_name]\n\nKeyError: 'pottedplant'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_30332]",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-6a08d5d38ff8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = yolo_model.fit(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mN_EPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr_schedule\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) UNKNOWN:  KeyError: 'pottedplant'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-42-54d41e2b71be>\", line 26, in parse_xml\n    class_dict[class_name]\n\nKeyError: 'pottedplant'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n\t [[yolo_loss/cond_1/else/_13/yolo_loss/cond_1/StringJoin/_92]]\n  (1) UNKNOWN:  KeyError: 'pottedplant'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 270, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n\n  File \"<ipython-input-42-54d41e2b71be>\", line 26, in parse_xml\n    class_dict[class_name]\n\nKeyError: 'pottedplant'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_train_function_30332]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.3918919 0.564     0.7777778 0.872     2.       ]]\n",
            "[[ 0.59066665  0.653       0.264       0.33       14.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aD5JHnCgPxgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KVN0RuCyPxdW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H-CfG1h8PxaJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}